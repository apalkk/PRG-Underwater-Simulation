{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: import files from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNOw3s54JxPT",
        "outputId": "1875ffca-31f7-456d-d350-5f6e09ea6d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rtree"
      ],
      "metadata": {
        "id": "jpbZjDZsO3em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5b2e21-80ba-474d-c39f-136a8ab6b38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rtree\n",
            "  Downloading Rtree-1.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (535 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.2/535.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rtree\n",
            "Successfully installed rtree-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "fJtlM9JTKJII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_poses = np.load(\"/content/drive/MyDrive/PRG/all_cameras_data.npz\")"
      ],
      "metadata": {
        "id": "1J_EeYjvQoe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_poses1 = np.load(\"/content/drive/MyDrive/PRG/cameras_sphere.npz\")"
      ],
      "metadata": {
        "id": "pG3p4CiMKv19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(processed_poses1.keys())"
      ],
      "metadata": {
        "id": "6Koa3jfzKyu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad25c4c0-ce36-421a-f163-0a6fe95bf988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['camera_mat_0',\n",
              " 'camera_mat_inv_0',\n",
              " 'world_mat_0',\n",
              " 'world_mat_inv_0',\n",
              " 'camera_mat_1',\n",
              " 'camera_mat_inv_1',\n",
              " 'world_mat_1',\n",
              " 'world_mat_inv_1',\n",
              " 'camera_mat_2',\n",
              " 'camera_mat_inv_2',\n",
              " 'world_mat_2',\n",
              " 'world_mat_inv_2',\n",
              " 'camera_mat_3',\n",
              " 'camera_mat_inv_3',\n",
              " 'world_mat_3',\n",
              " 'world_mat_inv_3',\n",
              " 'camera_mat_4',\n",
              " 'camera_mat_inv_4',\n",
              " 'world_mat_4',\n",
              " 'world_mat_inv_4',\n",
              " 'camera_mat_5',\n",
              " 'camera_mat_inv_5',\n",
              " 'world_mat_5',\n",
              " 'world_mat_inv_5',\n",
              " 'camera_mat_6',\n",
              " 'camera_mat_inv_6',\n",
              " 'world_mat_6',\n",
              " 'world_mat_inv_6',\n",
              " 'camera_mat_7',\n",
              " 'camera_mat_inv_7',\n",
              " 'world_mat_7',\n",
              " 'world_mat_inv_7',\n",
              " 'camera_mat_8',\n",
              " 'camera_mat_inv_8',\n",
              " 'world_mat_8',\n",
              " 'world_mat_inv_8',\n",
              " 'camera_mat_9',\n",
              " 'camera_mat_inv_9',\n",
              " 'world_mat_9',\n",
              " 'world_mat_inv_9',\n",
              " 'camera_mat_10',\n",
              " 'camera_mat_inv_10',\n",
              " 'world_mat_10',\n",
              " 'world_mat_inv_10',\n",
              " 'camera_mat_11',\n",
              " 'camera_mat_inv_11',\n",
              " 'world_mat_11',\n",
              " 'world_mat_inv_11',\n",
              " 'camera_mat_12',\n",
              " 'camera_mat_inv_12',\n",
              " 'world_mat_12',\n",
              " 'world_mat_inv_12',\n",
              " 'camera_mat_13',\n",
              " 'camera_mat_inv_13',\n",
              " 'world_mat_13',\n",
              " 'world_mat_inv_13',\n",
              " 'camera_mat_14',\n",
              " 'camera_mat_inv_14',\n",
              " 'world_mat_14',\n",
              " 'world_mat_inv_14',\n",
              " 'camera_mat_15',\n",
              " 'camera_mat_inv_15',\n",
              " 'world_mat_15',\n",
              " 'world_mat_inv_15',\n",
              " 'camera_mat_16',\n",
              " 'camera_mat_inv_16',\n",
              " 'world_mat_16',\n",
              " 'world_mat_inv_16',\n",
              " 'camera_mat_17',\n",
              " 'camera_mat_inv_17',\n",
              " 'world_mat_17',\n",
              " 'world_mat_inv_17',\n",
              " 'camera_mat_18',\n",
              " 'camera_mat_inv_18',\n",
              " 'world_mat_18',\n",
              " 'world_mat_inv_18',\n",
              " 'camera_mat_19',\n",
              " 'camera_mat_inv_19',\n",
              " 'world_mat_19',\n",
              " 'world_mat_inv_19',\n",
              " 'camera_mat_20',\n",
              " 'camera_mat_inv_20',\n",
              " 'world_mat_20',\n",
              " 'world_mat_inv_20',\n",
              " 'camera_mat_21',\n",
              " 'camera_mat_inv_21',\n",
              " 'world_mat_21',\n",
              " 'world_mat_inv_21',\n",
              " 'camera_mat_22',\n",
              " 'camera_mat_inv_22',\n",
              " 'world_mat_22',\n",
              " 'world_mat_inv_22',\n",
              " 'camera_mat_23',\n",
              " 'camera_mat_inv_23',\n",
              " 'world_mat_23',\n",
              " 'world_mat_inv_23',\n",
              " 'camera_mat_24',\n",
              " 'camera_mat_inv_24',\n",
              " 'world_mat_24',\n",
              " 'world_mat_inv_24',\n",
              " 'camera_mat_25',\n",
              " 'camera_mat_inv_25',\n",
              " 'world_mat_25',\n",
              " 'world_mat_inv_25',\n",
              " 'camera_mat_26',\n",
              " 'camera_mat_inv_26',\n",
              " 'world_mat_26',\n",
              " 'world_mat_inv_26',\n",
              " 'camera_mat_27',\n",
              " 'camera_mat_inv_27',\n",
              " 'world_mat_27',\n",
              " 'world_mat_inv_27',\n",
              " 'camera_mat_28',\n",
              " 'camera_mat_inv_28',\n",
              " 'world_mat_28',\n",
              " 'world_mat_inv_28',\n",
              " 'camera_mat_29',\n",
              " 'camera_mat_inv_29',\n",
              " 'world_mat_29',\n",
              " 'world_mat_inv_29',\n",
              " 'camera_mat_30',\n",
              " 'camera_mat_inv_30',\n",
              " 'world_mat_30',\n",
              " 'world_mat_inv_30',\n",
              " 'camera_mat_31',\n",
              " 'camera_mat_inv_31',\n",
              " 'world_mat_31',\n",
              " 'world_mat_inv_31',\n",
              " 'scale_mat_0',\n",
              " 'scale_mat_inv_0',\n",
              " 'scale_mat_1',\n",
              " 'scale_mat_inv_1',\n",
              " 'scale_mat_2',\n",
              " 'scale_mat_inv_2',\n",
              " 'scale_mat_3',\n",
              " 'scale_mat_inv_3',\n",
              " 'scale_mat_4',\n",
              " 'scale_mat_inv_4',\n",
              " 'scale_mat_5',\n",
              " 'scale_mat_inv_5',\n",
              " 'scale_mat_6',\n",
              " 'scale_mat_inv_6',\n",
              " 'scale_mat_7',\n",
              " 'scale_mat_inv_7',\n",
              " 'scale_mat_8',\n",
              " 'scale_mat_inv_8',\n",
              " 'scale_mat_9',\n",
              " 'scale_mat_inv_9',\n",
              " 'scale_mat_10',\n",
              " 'scale_mat_inv_10',\n",
              " 'scale_mat_11',\n",
              " 'scale_mat_inv_11',\n",
              " 'scale_mat_12',\n",
              " 'scale_mat_inv_12',\n",
              " 'scale_mat_13',\n",
              " 'scale_mat_inv_13',\n",
              " 'scale_mat_14',\n",
              " 'scale_mat_inv_14',\n",
              " 'scale_mat_15',\n",
              " 'scale_mat_inv_15',\n",
              " 'scale_mat_16',\n",
              " 'scale_mat_inv_16',\n",
              " 'scale_mat_17',\n",
              " 'scale_mat_inv_17',\n",
              " 'scale_mat_18',\n",
              " 'scale_mat_inv_18',\n",
              " 'scale_mat_19',\n",
              " 'scale_mat_inv_19',\n",
              " 'scale_mat_20',\n",
              " 'scale_mat_inv_20',\n",
              " 'scale_mat_21',\n",
              " 'scale_mat_inv_21',\n",
              " 'scale_mat_22',\n",
              " 'scale_mat_inv_22',\n",
              " 'scale_mat_23',\n",
              " 'scale_mat_inv_23',\n",
              " 'scale_mat_24',\n",
              " 'scale_mat_inv_24',\n",
              " 'scale_mat_25',\n",
              " 'scale_mat_inv_25',\n",
              " 'scale_mat_26',\n",
              " 'scale_mat_inv_26',\n",
              " 'scale_mat_27',\n",
              " 'scale_mat_inv_27',\n",
              " 'scale_mat_28',\n",
              " 'scale_mat_inv_28',\n",
              " 'scale_mat_29',\n",
              " 'scale_mat_inv_29',\n",
              " 'scale_mat_30',\n",
              " 'scale_mat_inv_30',\n",
              " 'scale_mat_31',\n",
              " 'scale_mat_inv_31']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(processed_poses.keys())"
      ],
      "metadata": {
        "id": "za8xpidDQxuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228096b3-5794-49f0-ba4a-938199164390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cam_mat_0',\n",
              " 'camera_mat_inv_0',\n",
              " 'world_mat_0',\n",
              " 'world_mat_inv_0',\n",
              " 'scale_mat_0',\n",
              " 'cam_mat_1',\n",
              " 'camera_mat_inv_1',\n",
              " 'world_mat_1',\n",
              " 'world_mat_inv_1',\n",
              " 'scale_mat_1',\n",
              " 'cam_mat_2',\n",
              " 'camera_mat_inv_2',\n",
              " 'world_mat_2',\n",
              " 'world_mat_inv_2',\n",
              " 'scale_mat_2',\n",
              " 'cam_mat_3',\n",
              " 'camera_mat_inv_3',\n",
              " 'world_mat_3',\n",
              " 'world_mat_inv_3',\n",
              " 'scale_mat_3',\n",
              " 'cam_mat_4',\n",
              " 'camera_mat_inv_4',\n",
              " 'world_mat_4',\n",
              " 'world_mat_inv_4',\n",
              " 'scale_mat_4',\n",
              " 'cam_mat_5',\n",
              " 'camera_mat_inv_5',\n",
              " 'world_mat_5',\n",
              " 'world_mat_inv_5',\n",
              " 'scale_mat_5',\n",
              " 'cam_mat_6',\n",
              " 'camera_mat_inv_6',\n",
              " 'world_mat_6',\n",
              " 'world_mat_inv_6',\n",
              " 'scale_mat_6',\n",
              " 'cam_mat_7',\n",
              " 'camera_mat_inv_7',\n",
              " 'world_mat_7',\n",
              " 'world_mat_inv_7',\n",
              " 'scale_mat_7',\n",
              " 'cam_mat_8',\n",
              " 'camera_mat_inv_8',\n",
              " 'world_mat_8',\n",
              " 'world_mat_inv_8',\n",
              " 'scale_mat_8',\n",
              " 'cam_mat_9',\n",
              " 'camera_mat_inv_9',\n",
              " 'world_mat_9',\n",
              " 'world_mat_inv_9',\n",
              " 'scale_mat_9',\n",
              " 'cam_mat_10',\n",
              " 'camera_mat_inv_10',\n",
              " 'world_mat_10',\n",
              " 'world_mat_inv_10',\n",
              " 'scale_mat_10',\n",
              " 'cam_mat_11',\n",
              " 'camera_mat_inv_11',\n",
              " 'world_mat_11',\n",
              " 'world_mat_inv_11',\n",
              " 'scale_mat_11',\n",
              " 'cam_mat_12',\n",
              " 'camera_mat_inv_12',\n",
              " 'world_mat_12',\n",
              " 'world_mat_inv_12',\n",
              " 'scale_mat_12',\n",
              " 'cam_mat_13',\n",
              " 'camera_mat_inv_13',\n",
              " 'world_mat_13',\n",
              " 'world_mat_inv_13',\n",
              " 'scale_mat_13',\n",
              " 'cam_mat_14',\n",
              " 'camera_mat_inv_14',\n",
              " 'world_mat_14',\n",
              " 'world_mat_inv_14',\n",
              " 'scale_mat_14',\n",
              " 'cam_mat_15',\n",
              " 'camera_mat_inv_15',\n",
              " 'world_mat_15',\n",
              " 'world_mat_inv_15',\n",
              " 'scale_mat_15',\n",
              " 'cam_mat_16',\n",
              " 'camera_mat_inv_16',\n",
              " 'world_mat_16',\n",
              " 'world_mat_inv_16',\n",
              " 'scale_mat_16',\n",
              " 'cam_mat_17',\n",
              " 'camera_mat_inv_17',\n",
              " 'world_mat_17',\n",
              " 'world_mat_inv_17',\n",
              " 'scale_mat_17',\n",
              " 'cam_mat_18',\n",
              " 'camera_mat_inv_18',\n",
              " 'world_mat_18',\n",
              " 'world_mat_inv_18',\n",
              " 'scale_mat_18',\n",
              " 'cam_mat_19',\n",
              " 'camera_mat_inv_19',\n",
              " 'world_mat_19',\n",
              " 'world_mat_inv_19',\n",
              " 'scale_mat_19',\n",
              " 'cam_mat_20',\n",
              " 'camera_mat_inv_20',\n",
              " 'world_mat_20',\n",
              " 'world_mat_inv_20',\n",
              " 'scale_mat_20',\n",
              " 'cam_mat_21',\n",
              " 'camera_mat_inv_21',\n",
              " 'world_mat_21',\n",
              " 'world_mat_inv_21',\n",
              " 'scale_mat_21',\n",
              " 'cam_mat_22',\n",
              " 'camera_mat_inv_22',\n",
              " 'world_mat_22',\n",
              " 'world_mat_inv_22',\n",
              " 'scale_mat_22',\n",
              " 'cam_mat_23',\n",
              " 'camera_mat_inv_23',\n",
              " 'world_mat_23',\n",
              " 'world_mat_inv_23',\n",
              " 'scale_mat_23',\n",
              " 'cam_mat_24',\n",
              " 'camera_mat_inv_24',\n",
              " 'world_mat_24',\n",
              " 'world_mat_inv_24',\n",
              " 'scale_mat_24',\n",
              " 'cam_mat_25',\n",
              " 'camera_mat_inv_25',\n",
              " 'world_mat_25',\n",
              " 'world_mat_inv_25',\n",
              " 'scale_mat_25',\n",
              " 'cam_mat_26',\n",
              " 'camera_mat_inv_26',\n",
              " 'world_mat_26',\n",
              " 'world_mat_inv_26',\n",
              " 'scale_mat_26']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_poses['world_mat_13']"
      ],
      "metadata": {
        "id": "w38DPyfBQz0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_mesh(scene_or_mesh):\n",
        "    \"\"\"\n",
        "    Convert a possible scene to a mesh.\n",
        "\n",
        "    If conversion occurs, the returned mesh has only vertex and face data.\n",
        "    \"\"\"\n",
        "    if isinstance(scene_or_mesh, trimesh.Scene):\n",
        "        if len(scene_or_mesh.geometry) == 0:\n",
        "            mesh = None  # empty scene\n",
        "        else:\n",
        "            # we lose texture information here\n",
        "            mesh = trimesh.util.concatenate(\n",
        "                tuple(trimesh.Trimesh(vertices=g.vertices, faces=g.faces)\n",
        "                    for g in scene_or_mesh.geometry.values()))\n",
        "    else:\n",
        "        assert(isinstance(scene_or_mesh, trimesh.Trimesh))\n",
        "        mesh = scene_or_mesh\n",
        "    return mesh"
      ],
      "metadata": {
        "id": "X02Ai6I3anr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh"
      ],
      "metadata": {
        "id": "NO8vxMaIa3TH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49d9de4-8e7d-475f-9aa8-097fe87ef918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.4.1-py3-none-any.whl (694 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.7/694.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.25.2)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-4.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh"
      ],
      "metadata": {
        "id": "J9SLMnDLaoc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mesh = trimesh.load(\"/content/drive/MyDrive/PRG/camera_scene.obj\")\n",
        "mesh = as_mesh(mesh)"
      ],
      "metadata": {
        "id": "JXPF_xNfatba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mesh.vertices"
      ],
      "metadata": {
        "id": "1pdsylLhiOhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f02a5b-eb68-4613-b8e5-ae861bc89f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrackedArray([[-2.38636000e-01, -1.47250000e-01, -7.13768000e-01],\n",
              "              [-2.10137000e-01, -1.47250000e-01, -6.85871000e-01],\n",
              "              [-1.81639000e-01, -1.47250000e-01, -6.57975000e-01],\n",
              "              ...,\n",
              "              [ 2.63827271e+02,  0.00000000e+00, -2.79249268e+02],\n",
              "              [ 2.53827240e+02,  0.00000000e+00, -2.89249268e+02],\n",
              "              [ 2.63827271e+02,  0.00000000e+00, -2.89249268e+02]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cvt_blender_to_neus_coords(pts):\n",
        "    original_shape = pts.shape\n",
        "    pts = pts.reshape(-1, 3)\n",
        "    cmat = np.array(\n",
        "        [[1.0, 0.0, 0.0],\n",
        "        [0.0, 0.0, -1.0],\n",
        "        [0.0, 1.0, 0.0]])\n",
        "    out = pts @ cmat.T\n",
        "    return out.reshape(original_shape)"
      ],
      "metadata": {
        "id": "kvOclU2Mkrud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mesh.vertices = cvt_blender_to_neus_coords(mesh.vertices)"
      ],
      "metadata": {
        "id": "VCsNBPWrksaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "RZV30TFek7Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_neus_cams(cam_path):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        cam_path: path to cameras_sphere.npz file\n",
        "    Returns:\n",
        "        poses: (n, 3) torch tensor of unit-sphere (relative to scene) scaled camera poses\n",
        "    \"\"\"\n",
        "    poses = []\n",
        "    processed_poses = np.load(cam_path)\n",
        "    for k in processed_poses.keys():\n",
        "        if \"camera_mat_inv\" in k:\n",
        "            camera_mat_inv = processed_poses[k]\n",
        "        if \"world_mat\" in k and \"world_mat_inv\" not in k:\n",
        "            world_mat = processed_poses[k]\n",
        "            poses.append(np.linalg.inv(camera_mat_inv @ world_mat))\n",
        "    poses = np.array(poses)\n",
        "\n",
        "    scale_mat = processed_poses[\"scale_mat_0\"]\n",
        "    scale = scale_mat[0, 0]\n",
        "    offset = scale_mat[:3, 3]\n",
        "    poses[:, :3, -1] = (poses[:, :3, -1] - offset) / scale\n",
        "    poses = torch.from_numpy(poses)\n",
        "    return poses, scale, offset"
      ],
      "metadata": {
        "id": "AErLWYoekzpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poses, scale, offset = load_neus_cams(\"/content/drive/MyDrive/PRG/all_cameras_data.npz\")"
      ],
      "metadata": {
        "id": "83KYDDs0k4y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sonar_project_hitpoints(locations,\n",
        "                            index_tri,\n",
        "                            index_ray,\n",
        "                            mesh,\n",
        "                            origin,\n",
        "                            rays_d,\n",
        "                            cam_info,):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        origin: (1, 3) np.ndarray\n",
        "    \"\"\"\n",
        "    H, W = cam_info[\"rad_bins\"], cam_info[\"azi_bins\"]\n",
        "    near, far = cam_info[\"rad_range\"]\n",
        "    pp_arc = cam_info[\"pp_arc\"]\n",
        "    dists = np.linalg.norm(locations - origin, axis=-1)\n",
        "    r_res = (far - near) / H\n",
        "    r_binned = ((dists - near) / r_res).astype(int)\n",
        "    a_binned = (index_ray / pp_arc).astype(int)\n",
        "\n",
        "    shading = -(mesh.face_normals[index_tri] * rays_d[index_ray]).sum(axis=-1)\n",
        "\n",
        "    total = np.zeros((H, W))\n",
        "    counts = np.zeros((H, W))\n",
        "    render_rt = np.zeros((H, W))\n",
        "    rad_img = np.zeros((H, W))\n",
        "    print(\"r_binned\", r_binned.shape)\n",
        "    print(\"a_binned\", a_binned.shape)\n",
        "    print(\"H\", H)\n",
        "    print(\"W\", W)\n",
        "    print(rad_img.shape)\n",
        "    rad_img[r_binned, a_binned] += dists\n",
        "\n",
        "    # mask[r_binned, a_binned] = 1\n",
        "    np.add.at(total, (r_binned, a_binned), shading)\n",
        "    np.add.at(counts, (r_binned, a_binned), 1)\n",
        "    # remember to average over the number of rays per bin?\n",
        "    render_rt[counts > 0] = total[counts > 0] / counts[counts > 0]\n",
        "    render_rt[counts > 0] = render_rt[counts > 0] / rad_img[counts > 0]\n",
        "    render_rt_out = (render_rt*255).astype(np.uint8)\n",
        "\n",
        "    return render_rt_out"
      ],
      "metadata": {
        "id": "Bz2CwXP4MUCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rays_at_sonar_for_proj(\n",
        "    pose, azi_range, azi_bins, ele_range, pp_arc, **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        rays_o: (azi_bins*pp_arc, 3), numpy array\n",
        "        rays_d: (azi_bins*pp_arc, 3), numpy array\n",
        "    \"\"\"\n",
        "    azis = torch.linspace(azi_range[0], azi_range[1], azi_bins)\n",
        "    eles = torch.linspace(ele_range[0], ele_range[1], pp_arc)\n",
        "    pixels_theta, pixels_phi = torch.meshgrid(\n",
        "        azis, eles, indexing=\"ij\"  # careful with indexing here\n",
        "    )  # azi_bins, pp_arc\n",
        "    xs = -torch.cos(pixels_phi) * torch.sin(pixels_theta)\n",
        "    ys = -torch.sin(pixels_phi)  # sign?\n",
        "    zs = torch.cos(pixels_phi) * torch.cos(pixels_theta)\n",
        "    p = torch.stack([xs, ys, zs], axis=-1)\n",
        "\n",
        "    rays_v = p / torch.linalg.norm(\n",
        "        p, ord=2, dim=-1, keepdims=True\n",
        "    )  # azi_bins, pp_arc, 3\n",
        "    rays_v = torch.matmul(pose[None, None, :3, :3], rays_v[..., None]).squeeze()\n",
        "    rays_o = pose[None, :3, 3].expand(\n",
        "        rays_v.shape\n",
        "    )  # azi_bins, pp_arc, 3\n",
        "    # np.savez(\"/content/drive/MyDrive/PRG/origin.npz\", rays_o.reshape(-1, 3))\n",
        "    # np.savez(\"/content/drive/MyDrive/PRG/direction.npz\", rays_v.reshape(-1, 3))\n",
        "    return rays_o.reshape(-1, 3).detach().cpu().numpy(), rays_v.reshape(-1, 3).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "A1KQ7OTkMIg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_ray_intersections(mesh, ray_origins, ray_directions, batch_size=1000):\n",
        "    num_rays = len(ray_origins)\n",
        "    locations = []\n",
        "    index_ray = []\n",
        "    index_tri = []\n",
        "\n",
        "    for start in range(0, num_rays, batch_size):\n",
        "        end = min(start + batch_size, num_rays)\n",
        "        loc, i_ray, i_tri = mesh.ray.intersects_location(\n",
        "            ray_origins=ray_origins[start:end],\n",
        "            ray_directions=ray_directions[start:end],\n",
        "            multiple_hits=False)\n",
        "\n",
        "        if loc.shape[0] > 0:\n",
        "            locations.append(loc)\n",
        "            index_ray.append(i_ray)\n",
        "            index_tri.append(i_tri)\n",
        "\n",
        "    if locations:\n",
        "        locations = np.concatenate(locations)\n",
        "    else:\n",
        "        locations = np.empty((0, 3))\n",
        "\n",
        "    if index_ray:\n",
        "        index_ray = np.concatenate(index_ray)\n",
        "    else:\n",
        "        index_ray = np.empty((0,), dtype=int)\n",
        "\n",
        "    if index_tri:\n",
        "        index_tri = np.concatenate(index_tri)\n",
        "    else:\n",
        "        index_tri = np.empty((0,), dtype=int)\n",
        "\n",
        "    return locations, index_ray, index_tri\n"
      ],
      "metadata": {
        "id": "U3icMA8rzJJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_sonar_image(pose, cam_info, mesh):\n",
        "\n",
        "    # below has issues with multiprocessing\n",
        "    ray_origins, ray_directions = gen_rays_at_sonar_for_proj(pose, **cam_info)  # rad_bins, azi_bins, pp_arc, 3\n",
        "\n",
        "    locations, index_ray, index_tri = mesh.ray.intersects_location(\n",
        "        ray_origins=ray_origins,\n",
        "        ray_directions=ray_directions,\n",
        "        multiple_hits=False) # (n, 3), (n,), (n,)\n",
        "\n",
        "    # locations, index_ray, index_tri = batch_ray_intersections(mesh, ray_origins, ray_directions)\n",
        "\n",
        "    origin = pose[:3, -1].cpu().numpy()\n",
        "    render_rt = sonar_project_hitpoints(locations,\n",
        "                                        index_tri,\n",
        "                                        index_ray, mesh, origin, ray_directions, cam_info)\n",
        "    # print(render_rt.sum())\n",
        "    return render_rt"
      ],
      "metadata": {
        "id": "nw8bJCXUMG7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import math"
      ],
      "metadata": {
        "id": "RFcx_o8WMkJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: vfov for a camera sensor\n",
        "\n",
        "# Calculate vertical field of view (vfov)\n",
        "sensor_height = 24.0  # Replace with actual sensor height in mm\n",
        "focal_length = 36.0  # Replace with actual focal length in mm\n",
        "\n",
        "vfov = 2 * math.atan(sensor_height / (2 * focal_length))\n",
        "vfov_degrees = math.degrees(vfov)\n",
        "\n",
        "print(\"Vertical Field of View (vfov):\", vfov_degrees, \"degrees\")\n"
      ],
      "metadata": {
        "id": "bSTLiPWhM7xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d57bf9-7589-48e6-8f43-637313b1b63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertical Field of View (vfov): 36.86989764584402 degrees\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ele_min = math.radians(-vfov_degrees/2) # vfov - vertical field of view of camera sensor\n",
        "ele_max = math.radians(vfov_degrees/2)"
      ],
      "metadata": {
        "id": "lFFx7BmxLwNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam_info = dict(\n",
        "        azi_range=[-np.pi / 6.0, np.pi / 6.0],\n",
        "        azi_bins=1024,  # W\n",
        "        rad_range=[0.01, 4.0],\n",
        "        rad_bins=1024, # H\n",
        "        ele_range=[ele_min, ele_max],\n",
        "        pp_arc=512,\n",
        "    )\n",
        "\n",
        "# cam_info = dict(\n",
        "#     azi_range=[-np.pi / 6.0, np.pi / 6.0],\n",
        "#     azi_bins=48,  # reduced azimuthal number of bins\n",
        "#     rad_range=[0.01, 4.0],\n",
        "#     rad_bins=64,  # reduced radial number of bins\n",
        "#     ele_range=[ele_min, ele_max],\n",
        "#     pp_arc=512,  # reduced points per arc\n",
        "# )\n"
      ],
      "metadata": {
        "id": "QaD12nZqLDJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = \"/content/drive/MyDrive/PRG/out\""
      ],
      "metadata": {
        "id": "_079ruz_NO96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: need a function convert_pose that modifies pose from \"neus to ho\" based on\n",
        "# def cvt_neus_coords_to_ho_coords(pts):\n",
        "#     \"\"\"\n",
        "#     args:\n",
        "#         pts: (..., 3)\n",
        "#     \"\"\"\n",
        "#     original_shape = pts.shape\n",
        "#     pts = pts.reshape(-1, 3)\n",
        "#     cmat = np.array(\n",
        "#         [[0.0, 0.0, 1.0],\n",
        "#         [-1.0, 0.0, 0.0],\n",
        "#         [0.0, -1.0, 0.0]])\n",
        "#     out = pts @ cmat.T\n",
        "#     return out.reshape(original_shape)\n",
        "\n",
        "def convert_pose(pose):\n",
        "  \"\"\"\n",
        "  Converts a pose from Neus coordinate system to HO coordinate system.\n",
        "\n",
        "  Args:\n",
        "    pose: (4, 4) transformation matrix in Neus coordinates.\n",
        "\n",
        "  Returns:\n",
        "    (4, 4) transformation matrix in HO coordinates.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract rotation and translation from the pose\n",
        "  rotation = pose[:3, :3]\n",
        "  translation = pose[:3, 3]\n",
        "\n",
        "  # Convert rotation to HO coordinates\n",
        "  cmat = np.array([[0.0, 0.0, 1.0],\n",
        "                   [-1.0, 0.0, 0.0],\n",
        "                   [0.0, -1.0, 0.0]])\n",
        "  rotation_ho = cmat @ rotation @ cmat.T\n",
        "\n",
        "  # Convert translation to HO coordinates\n",
        "  translation_ho = cmat @ translation\n",
        "\n",
        "  # Construct the transformed pose in HO coordinates\n",
        "  pose_ho = np.eye(4)\n",
        "  pose_ho[:3, :3] = rotation_ho\n",
        "  pose_ho[:3, 3] = translation_ho\n",
        "\n",
        "  return pose_ho\n"
      ],
      "metadata": {
        "id": "HFbS6oAbNW1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "aLQJQtVUOB1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(poses)):\n",
        "        pose = poses[i]\n",
        "        # rotate sonar by 90 degrees, about z (optical) axis\n",
        "        rot = torch.tensor([[0.0, -1.0, 0.0],\n",
        "                            [1.0, 0.0, 0.0],\n",
        "                            [0.0, 0.0, 1.0]])\n",
        "        # pose = pose\n",
        "        # rot = rot.to(device)\n",
        "        pose[:3, :3] = pose[:3, :3] @ rot\n",
        "        render_rt = render_sonar_image(pose, cam_info, mesh)\n",
        "        print(render_rt.sum())\n",
        "        if render_rt.sum() > 10:\n",
        "            cv2.imwrite(f\"{out_dir}/imgs/{i:0>4d}.png\", render_rt)\n",
        "\n",
        "            curr_data = {}\n",
        "            # curr_pose = convert_pose(pose.cpu().numpy(), \"neus_to_ho\")\n",
        "            # curr_data[\"PoseSensor\"] = curr_pose\n",
        "            curr_data[\"ImagingSonar\"] = render_rt\n",
        "            with open(f\"{out_dir}/Data/{i:03d}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(curr_data, f)"
      ],
      "metadata": {
        "id": "pUTyNR4jlJII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def export_npz(file_path, object_name=\"BlueROV\"):\n",
        "#     cam = bpy.data.objects[camera_name].data\n",
        "#     scene = bpy.context.scene\n",
        "#     f_in_mm = np.float32(cam.lens)\n",
        "#     sensor_width_in_mm = np.float32(cam.sensor_width)\n",
        "#     w = np.float32(scene.render.resolution_x)\n",
        "#     h = np.float32(scene.render.resolution_y)\n",
        "#     pixel_aspect = np.float32(scene.render.pixel_aspect_y / scene.render.pixel_aspect_x)\n",
        "#     f_x = np.float32(f_in_mm / sensor_width_in_mm * w)\n",
        "#     f_y = np.float32(f_x * pixel_aspect)\n",
        "#     c_x = np.float32(w * (0.5 - cam.shift_x))\n",
        "#     c_y = np.float32(h * 0.5 + w * cam.shift_y)\n",
        "#     cam_mat = np.array([\n",
        "#         [f_x, 0, c_x, 0],\n",
        "#         [0, f_y, c_y, 0],\n",
        "#         [0, 0, 1, 0],\n",
        "#         [0, 0, 0, 1]\n",
        "#     ], dtype=np.float32)\n",
        "#     cam_inv = np.linalg.inv(cam_mat).astype(np.float32)\n",
        "#     obj = bpy.data.objects[object_name]\n",
        "#     world_mat = np.array(obj.matrix_world, dtype=np.float32)\n",
        "#     world_mat_inv = np.linalg.inv(world_mat).astype(np.float32)\n",
        "#     scale_mat = np.array([\n",
        "#         [1, 0, 0, 0],\n",
        "#         [0, 1, 0, 0],\n",
        "#         [0, 0, 1, 0],\n",
        "#         [0, 0, 0, 1]\n",
        "#     ], dtype=np.float32)\n",
        "#     np.savez(file_path, cam_mat_0=cam_mat, camera_mat_inv_0=cam_inv, world_mat_0=world_mat, world_mat_inv_0=world_mat_inv, scale_mat_0=scale_mat)"
      ],
      "metadata": {
        "id": "FJalAhhSCo7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import glob\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# def read_images_from_folder(folder_path, file_extension='png'):\n",
        "#     # Get list of image files in the folder\n",
        "#     image_files = glob.glob(os.path.join(folder_path, f'*.{file_extension}'))\n",
        "#     images = []\n",
        "\n",
        "#     for file in image_files:\n",
        "#         # Read image using OpenCV\n",
        "#         image = cv2.imread(file, cv2.IMREAD_UNCHANGED)  # Reads the image with alpha channel if present\n",
        "#         if image is not None:\n",
        "#             images.append(image)\n",
        "#         else:\n",
        "#             print(f\"Warning: {file} could not be read.\")\n",
        "\n",
        "#     return images\n",
        "\n",
        "# def verify_unique_pixel_values(images):\n",
        "#     for i, image in enumerate(images):\n",
        "#         # Flatten the image array to check for unique values\n",
        "#         flat_image = image.flatten()\n",
        "#         print(\"flat_image is\", flat_image)\n",
        "#         unique_values = np.unique(flat_image)\n",
        "#         print(\"unique_values\", unique_values)\n",
        "#         if len(unique_values) != len(flat_image):\n",
        "#             print(f\"Image {i} does not have unique pixel values.\")\n",
        "#             # return False\n",
        "#         else:\n",
        "#             print(f\"Image {i} has unique pixel values.\")\n",
        "\n",
        "#     print(\"All images have unique pixel values.\")\n",
        "#     return True\n",
        "\n",
        "# # Main execution\n",
        "# folder_path = '/content/drive/MyDrive/PRG/out/imgs/camera_scene_24/'  # Replace with your folder path\n",
        "# file_extension = 'png'  # Replace with your image file extension (e.g., 'jpg', 'jpeg', 'bmp')\n",
        "\n",
        "# images = read_images_from_folder(folder_path, file_extension)\n",
        "# if verify_unique_pixel_values(images):\n",
        "#     print(\"Verification passed: All images have unique pixel values.\")\n",
        "# else:\n",
        "#     print(\"Verification failed: Some images do not have unique pixel values.\")\n"
      ],
      "metadata": {
        "id": "0ezqRdQ6sKN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def select_visible_objects(camera_name):\n",
        "#     cam = bpy.data.objects[camera_name]\n",
        "#     bpy.context.view_layer.objects.active = cam\n",
        "#     bpy.ops.object.select_all(action='DESELECT')\n",
        "#     cam.select_set(True)\n",
        "#     bpy.context.view_layer.objects.active = cam\n",
        "#     bpy.ops.object.mode_set(mode='OBJECT')\n",
        "#     bpy.ops.view3d.camera_to_view_selected()\n",
        "#     bpy.ops.object.select_all(action='SELECT')\n",
        "#     mesh_objects = [obj for obj in bpy.context.selected_objects if obj.type == 'MESH']\n",
        "#     for obj in mesh_objects:\n",
        "#         obj.select_set(True)\n",
        "#         bpy.context.view_layer.objects.active = obj\n",
        "#         mesh = bmesh.new()\n",
        "#         mesh.from_mesh(obj.data)\n",
        "#         bmesh.ops.delete(mesh, geom=mesh.faces[:], context='FACES')\n",
        "#         mesh.to_mesh(obj.data)\n",
        "#         mesh.free()"
      ],
      "metadata": {
        "id": "R67KAGcjGqmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}